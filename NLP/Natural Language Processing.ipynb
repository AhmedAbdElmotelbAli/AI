{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VwK5-9FIB-lu"
   },
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1kiO9kACE6s"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7QG7sxmoCIvN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wTfaCIzdCLPA"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UCK6vQ5QCQJe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qekztq71CixT"
   },
   "source": [
    "## Cleaning the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3342,
     "status": "ok",
     "timestamp": 1586421521761,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "8u_yXh9dCmEE",
    "outputId": "59406636-e0c4-4547-c918-42ae30c11642"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ahmed Abdel-\n",
      "[nltk_data]     Monem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = []\n",
    "for i in range(0, 1000):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 980 samples and 1000 outcomes>\n"
     ]
    }
   ],
   "source": [
    "corpus\n",
    "from nltk import FreqDist\n",
    "frequency_distribution = FreqDist(corpus)\n",
    "print(frequency_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('back', 5),\n",
       " ('return', 3),\n",
       " ('food good', 3),\n",
       " ('disappoint', 3),\n",
       " ('love place', 3),\n",
       " ('like', 2),\n",
       " ('delici', 2),\n",
       " ('go back', 2),\n",
       " ('food terribl', 2),\n",
       " ('mistak', 2),\n",
       " ('awesom', 2),\n",
       " ('would recommend place', 2),\n",
       " ('eat', 2),\n",
       " ('wow love place', 1),\n",
       " ('crust good', 1),\n",
       " ('tasti textur nasti', 1),\n",
       " ('stop late may bank holiday rick steve recommend love', 1),\n",
       " ('select menu great price', 1),\n",
       " ('get angri want damn pho', 1),\n",
       " ('honeslti tast fresh', 1),\n",
       " ('potato like rubber could tell made ahead time kept warmer', 1),\n",
       " ('fri great', 1),\n",
       " ('great touch', 1),\n",
       " ('servic prompt', 1),\n",
       " ('would go back', 1),\n",
       " ('cashier care ever say still end wayyy overpr', 1),\n",
       " ('tri cape cod ravoli chicken cranberri mmmm', 1),\n",
       " ('disgust pretti sure human hair', 1),\n",
       " ('shock sign indic cash', 1),\n",
       " ('highli recommend', 1),\n",
       " ('waitress littl slow servic', 1),\n",
       " ('place worth time let alon vega', 1),\n",
       " ('burritto blah', 1),\n",
       " ('food amaz', 1),\n",
       " ('servic also cute', 1),\n",
       " ('could care less interior beauti', 1),\n",
       " ('perform', 1),\n",
       " ('right red velvet cake ohhh stuff good', 1),\n",
       " ('never brought salad ask', 1),\n",
       " ('hole wall great mexican street taco friendli staff', 1),\n",
       " ('took hour get food tabl restaur food luke warm sever run around like total overwhelm',\n",
       "  1),\n",
       " ('worst salmon sashimi', 1),\n",
       " ('also combo like burger fri beer decent deal', 1),\n",
       " ('like final blow', 1),\n",
       " ('found place accid could happier', 1),\n",
       " ('seem like good quick place grab bite familiar pub food favor look elsewher',\n",
       "  1),\n",
       " ('overal like place lot', 1),\n",
       " ('redeem qualiti restaur inexpens', 1),\n",
       " ('ampl portion good price', 1),\n",
       " ('poor servic waiter made feel like stupid everi time came tabl', 1),\n",
       " ('first visit hiro delight', 1),\n",
       " ('servic suck', 1),\n",
       " ('shrimp tender moist', 1),\n",
       " ('deal good enough would drag establish', 1),\n",
       " ('hard judg whether side good gross melt styrofoam want eat fear get sick',\n",
       "  1),\n",
       " ('posit note server attent provid great servic', 1),\n",
       " ('frozen puck disgust worst peopl behind regist', 1),\n",
       " ('thing like prime rib dessert section', 1),\n",
       " ('bad food damn gener', 1),\n",
       " ('burger good beef cook right', 1),\n",
       " ('want sandwich go firehous', 1),\n",
       " ('side greek salad greek dress tasti pita hummu refresh', 1),\n",
       " ('order duck rare pink tender insid nice char outsid', 1),\n",
       " ('came run us realiz husband left sunglass tabl', 1),\n",
       " ('chow mein good', 1),\n",
       " ('horribl attitud toward custom talk one custom enjoy food', 1),\n",
       " ('portion huge', 1),\n",
       " ('love friendli server great food wonder imagin menu', 1),\n",
       " ('heart attack grill downtown vega absolut flat line excus restaur', 1),\n",
       " ('much seafood like string pasta bottom', 1),\n",
       " ('salad right amount sauc power scallop perfectli cook', 1),\n",
       " ('rip banana rip petrifi tasteless', 1),\n",
       " ('least think refil water struggl wave minut', 1),\n",
       " ('place receiv star appet', 1),\n",
       " ('cocktail handmad delici', 1),\n",
       " ('definit go back', 1),\n",
       " ('glad found place', 1),\n",
       " ('great food servic huge portion give militari discount', 1),\n",
       " ('alway great time do gringo', 1),\n",
       " ('updat went back second time still amaz', 1),\n",
       " ('got food appar never heard salt batter fish chewi', 1),\n",
       " ('great way finish great', 1),\n",
       " ('deal includ tast drink jeff went beyond expect', 1),\n",
       " ('realli realli good rice time', 1),\n",
       " ('servic meh', 1),\n",
       " ('took min get milkshak noth chocol milk', 1),\n",
       " ('guess known place would suck insid excalibur use common sens', 1),\n",
       " ('scallop dish quit appal valu well', 1),\n",
       " ('time bad custom servic', 1),\n",
       " ('sweet potato fri good season well', 1),\n",
       " ('today second time lunch buffet pretti good', 1),\n",
       " ('much good food vega feel cheat wast eat opportun go rice compani', 1),\n",
       " ('come like experienc underwhelm relationship parti wait person ask break',\n",
       "  1),\n",
       " ('walk place smell like old greas trap other eat', 1),\n",
       " ('turkey roast beef bland', 1),\n",
       " ('place', 1),\n",
       " ('pan cake everyon rave tast like sugari disast tailor palat six year old',\n",
       "  1),\n",
       " ('love pho spring roll oh yummi tri', 1),\n",
       " ('poor batter meat ratio made chicken tender unsatisfi', 1),\n",
       " ('say food amaz', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_distribution.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahmed mohamed omar x y z s \n",
    "1       0       1  0 0 0 0\n",
    "0       1       0  1 0 0 0 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CLqmAkANCp1-"
   },
   "source": [
    "## Creating the Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qroF7XcSCvY3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1500)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 1].values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DH_VjgPzC2cd"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQXYM5VzDDDI"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VkIq23vEDIPt"
   },
   "source": [
    "## Training the Naive Bayes model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 636,
     "status": "ok",
     "timestamp": 1586421527177,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "DS9oiDXXDRdI",
    "outputId": "e4c2c831-e9dd-45f5-a23b-dd70f59cedfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "cls = RandomForestClassifier()\n",
    "cls.fit(X_train,y_train)\n",
    "y_pred_2 = cls.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1JaRM7zXDWUy"
   },
   "source": [
    "## Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iif0CVhFDaMp"
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.715"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xoMltea5Dir1"
   },
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 644,
     "status": "ok",
     "timestamp": 1586421531359,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "Xj9IU6MxDnvo",
    "outputId": "e95d3393-8348-46c6-e276-0c14de07e682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55 42]\n",
      " [12 91]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM9tx3HllsdwqqTLZQw/zx5",
   "collapsed_sections": [],
   "name": "Natural Language Processing",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
